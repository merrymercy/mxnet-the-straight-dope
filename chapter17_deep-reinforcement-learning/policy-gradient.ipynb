{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "In this tutorial, we will present the idea of policy gradient, and show how to use gluon to implement a policy gradient agent.\n",
    "To make things easy, we only solve a simple environment CartPole in this jupyter notebook (4 mins on MacBook Pro). But the code for more complex environment like Atari game Pong is also provided. You can try it if you are interested.\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "We use the popular toolkit OpenAI Gym as our environment. Use the following commonds to install OpenAI gym.\n",
    "```bash\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "```\n",
    "Full documentation for the gym can be found on at [this website](https://gym.openai.com/).\n",
    "\n",
    "## Policy Gradient Theorem\n",
    "\n",
    "This section briefly introduces the theorem behind the update rule of policy gradient. We want to optimize our policy so that to maximize long-term reward.\n",
    "\n",
    "Consider a simple one-step MDP (Markov Decission Precess). It starts in state $s \\sim d(s)$, and termiates after one time-step with reward $r = \\mathcal{R}_{s,a}$. The expected reward is \n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[r]\n",
    "            = \\sum_{s \\in \\mathcal{S}}{d(s) \\sum_{a \\in \\mathcal{A}}{\\pi_\\theta(s,a)\\mathcal{R}_{s,a}}}$$\n",
    "\n",
    "Then we can use likelihood ratios to compute the policy gradient.\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum_{s \\in \\mathcal{S}}{d(s) \\sum_{a \\in \\mathcal{A}}{\\pi_\\theta(s,a) \\nabla_\\theta log(\\pi_\\theta(s,a))\\mathcal{R}_{s,a}}} = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta log(\\pi_\\theta(s,a)) r]$$\n",
    "\n",
    "By generalising the likelihood ratio approach to multi-step MDPs and replace instataneous reward $r$ by long-term value $Q^{\\pi_\\theta}(s,a)$, we can get policy gradient theorem.\n",
    "\n",
    "**Theorem** For any differentiable policy  $\\pi_\\theta(s,a)$, for any of the policy objective functions $J = J_1$, $J_{average\\_reward}$ or $\\frac{1}{1-\\gamma}J_{average\\_value}$ , the policy gradient is \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta log(\\pi_\\theta(s,a)) Q^{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "In practice, we just update our parameters according to this formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Implementation\n",
    "Here we use an algorithm called REINFORCE, which is a Monte-Carlo version of policy gradient. The idea is to sample some transitions and update parameters by policy gradient. We can use returned reward as an unbiased estimation of $Q^{\\pi_\\theta}(s,a)$ in the theorem above.\n",
    "\n",
    "First, we define a class to store the transitions (observation, action, reward) in a whole episode. They are the training data for policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "class Episode:\n",
    "    def __init__(self):\n",
    "        self.observation = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "\n",
    "    def append(self, s, a, r):\n",
    "        self.observation.append(s)\n",
    "        self.action.append(a)\n",
    "        self.reward.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the agent. It uses a multilayer perceptron with 200 hidden units as its policy network.\n",
    "When inferring action, we do a softmax over the output to get the policy $\\pi$ and sample an action from $\\pi$.\n",
    "\n",
    "As for trainning, we need do some pre-precess to the reward. An immediate reward is not only  resulted by a single action, but also by actions taken several steps before, so a discounted factor $\\lambda$ is introduced to assign discounted reward to previous actions. Besides, in order to reduce variance, it's a common trick to substract a baseline function from the reward. Here we only use a simple empirical mean as baseline, more advaned methods may also train a network to predict this baseline. Finally, we utilize the autograd in gluon to compute gradient. You can see there is a minus sign in the loss function. The reason is that the trainer minimizes loss by default, but we want to maximize the cumulative reward. Thus a minus sign is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_space, ctx):\n",
    "        self.action_number = action_space.n\n",
    "\n",
    "        # define a MLP\n",
    "        net = gluon.nn.Sequential()\n",
    "        with net.name_scope():\n",
    "            net.add(gluon.nn.Flatten())\n",
    "            net.add(gluon.nn.Dense(200, activation='relu'))\n",
    "            net.add(gluon.nn.Dense(self.action_number))\n",
    "\n",
    "        net.collect_params().initialize(mx.init.Xavier(magnitude=0.5), ctx=ctx)\n",
    "        self.trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                                    {'learning_rate': 1e-3})\n",
    "\n",
    "        self.net = net\n",
    "        self.ctx = ctx\n",
    "\n",
    "    def act(self, observation):\n",
    "        # transform a single observation to a batch, necessary for shape inference\n",
    "        observation = [observation]\n",
    "        obs = mx.nd.array(observation, ctx=self.ctx)\n",
    "\n",
    "        policy = self.net(obs)[0] # get the first item of return batch\n",
    "        policy = mx.nd.softmax(policy).asnumpy()\n",
    "\n",
    "        return np.random.choice(np.arange(self.action_number), p=policy)\n",
    "\n",
    "    def train(self, episode):\n",
    "        s, a, r = episode.observation, episode.action, episode.reward\n",
    "\n",
    "        # discount reward\n",
    "        discount = 0.99\n",
    "        keep = 0\n",
    "        for i in reversed(range(len(r))):\n",
    "            keep = keep * discount + r[i]\n",
    "            r[i] = keep\n",
    "\n",
    "        s = mx.nd.array(s, ctx=self.ctx)\n",
    "        a = mx.nd.array(a, ctx=self.ctx).one_hot(self.action_number)\n",
    "        r = mx.nd.array(r, ctx=self.ctx).reshape((len(r), 1))\n",
    "\n",
    "        # substract baseline\n",
    "        r -= mx.nd.mean(r)\n",
    "\n",
    "        # forward\n",
    "        with autograd.record():\n",
    "            output = self.net(s)\n",
    "            policy = mx.nd.softmax(output)\n",
    "            policy = mx.nd.clip(policy, 1e-6, 1 - 1e-6) # clip for log\n",
    "            log_policy = mx.nd.log(policy)\n",
    "            loss = -mx.nd.sum(r * log_policy * a)\n",
    "\n",
    "        loss.backward()\n",
    "        self.trainer.step(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the function to collect transitions by simulating an episode. Since the policy gradient is an on-policy method, we should collect transitions by following the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, render):\n",
    "    episode = Episode()\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        episode.append(observation, action, reward)\n",
    "\n",
    "        observation = next_observation\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get every part done, it is time to begin the training loop.  (4 mins on MacBook Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space : Box(4,)\n",
      "action space : Discrete(2)\n",
      "episode: 0  reward: 15.00  mean reward: 15.00\n",
      "episode: 50  reward: 50.00  mean reward: 19.05\n",
      "episode: 100  reward: 16.00  mean reward: 23.03\n",
      "episode: 150  reward: 75.00  mean reward: 30.98\n",
      "episode: 200  reward: 132.00  mean reward: 51.49\n",
      "episode: 250  reward: 500.00  mean reward: 131.17\n",
      "episode: 300  reward: 500.00  mean reward: 253.50\n",
      "episode: 350  reward: 500.00  mean reward: 333.18\n",
      "episode: 400  reward: 348.00  mean reward: 385.56\n",
      "episode: 450  reward: 500.00  mean reward: 425.69\n",
      "episode: 500  reward: 500.00  mean reward: 440.79\n",
      "episode: 550  reward: 483.00  mean reward: 455.47\n",
      "episode: 600  reward: 500.00  mean reward: 454.21\n",
      "episode: 650  reward: 500.00  mean reward: 471.28\n",
      "episode: 700  reward: 500.00  mean reward: 474.28\n",
      "episode: 750  reward: 500.00  mean reward: 482.53\n",
      "episode: 800  reward: 500.00  mean reward: 488.84\n",
      "episode: 850  reward: 500.00  mean reward: 489.73\n",
      "episode: 900  reward: 500.00  mean reward: 493.79\n",
      "episode: 950  reward: 500.00  mean reward: 495.51\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# init environment and agent\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = Agent(env.action_space, mx.cpu())\n",
    "\n",
    "print(\"observation space : {}\".format(env.observation_space))\n",
    "print(\"action space : {}\".format(env.action_space))\n",
    "\n",
    "n_episodes = 1000\n",
    "mean_reward = 0\n",
    "reward_log = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    # collect transitions\n",
    "    episode = run_episode(env, agent, render=False)\n",
    "    reward = sum(episode.reward)\n",
    "\n",
    "    # save moving average of episode reward for plotting\n",
    "    if i == 0:\n",
    "        mean_reward = reward\n",
    "    else:\n",
    "        mean_reward = 0.99 * mean_reward + 0.01 * reward\n",
    "    reward_log.append(mean_reward)\n",
    "\n",
    "    # train the agent\n",
    "    agent.train(episode)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"episode: %d  reward: %.2f  mean reward: %.2f\" %\n",
    "                (i, reward, mean_reward))\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curve\n",
    "The reward curve can be plotted as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47094a6990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot reward\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(reward_log)\n",
    "plt.legend([\"mean reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Environment\n",
    "In order to solve more complex environment like Atari game Pong. There are just several modifications you should do. The network part can even be the same, you only need to pre-process the input image of Atari games. The following is an example.\n",
    "But this brute force policy gradient will cost a lot of training time, you'd better try other advanced methods like A3C to save you time and get better performance.\n",
    "\n",
    "In fact, this tutorial reference some parts of [this blog post](http://karpathy.github.io/2016/05/31/rl/) by Andrej Karpathy. Our network structure are the same. So I also recommond you to read throught it and see what will happen when solving Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariPreprocessedEnv:\n",
    "    \"\"\" A warper for atari game environment.\n",
    "    To use this warpper, replace 'env = gym.make(\"CartPole-v1\")' with \n",
    "    'env = AtariPreprocessedEnv()' and paste the definition of this warpper\n",
    "    before that statement (class should be defined before use)\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"PongDeterministic-v0\"):\n",
    "        self.env = gym.make(name)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = (80*80,)\n",
    "\n",
    "        self.pre_image = None\n",
    "\n",
    "    def _preprocess(self, img):\n",
    "        \"\"\" transform a 210x160x3 uint8 image to a 6400x1 float vector \n",
    "        Crop, down-sample, erase background and set foreground to 1\n",
    "        ref: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "        \"\"\"\n",
    "        img = img[35:195]\n",
    "        img = img[::2, ::2, 0]\n",
    "        img[img == 144] = 0\n",
    "        img[img == 109] = 0\n",
    "        img[img != 0] = 1\n",
    "        curr = img.astype(np.float).ravel()\n",
    "        # Subtract the last preprocessed image.\n",
    "        diff = (curr - self.pre_image if self.pre_image is not None\n",
    "                else np.zeros_like(curr))\n",
    "        self.pre_image = curr\n",
    "        return diff\n",
    "\n",
    "    def reset(self):\n",
    "        self.pre_image = None\n",
    "        return self._preprocess(self.env.reset())\n",
    "\n",
    "    def step(self, action):\n",
    "        o, r, d, i = self.env.step(action)\n",
    "        return self._preprocess(o), r, d, i\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
